{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsJWqIBpuIRp7reoMo96Sy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sym33/Colab/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = datasets.make_regression(n_samples=100, n_features=2, n_informative=2, noise=3, coef=False, random_state=333) \n"
      ],
      "metadata": {
        "id": "Oe5s_oCb4VWS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "### Grapheneinstieg mit Placeholder erstellen, ohne Initialwerte (None), mit 2 Attributen f체r x\n",
        "### Variablen enthalten dann die Tensoren! \n",
        "x = tf.compat.v1.placeholder(tf.float32, shape= (None, 2))\n",
        "y = tf.compat.v1.placeholder(tf.float32, shape=(None, 1)) \n",
        "\n",
        "### Variablen w und b, die initialisiert werden und die Operation w*x + b als Modell=output\n",
        "w = tf.Variable(tf.compat.v1.truncated_normal(shape= [2,1], stddev=0.5))\n",
        "b = tf.Variable(tf.compat.v1.random_normal([1]))\n",
        "output = tf.add(tf.compat.v1.matmul(x, w), b)\n",
        "\n",
        "### Kostenfunktion, wichtig: 0-Axe steht f체r Zeilen und 1-Axis f체r Spalten\n",
        "cost = tf.reduce_sum(tf.square(output-y)) \n",
        "\n",
        "### Nutze GradientDescent mit Lernrate 0.00001, die auf die Kostenfunktion angewendet wird\n",
        "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(cost)\n",
        "\n",
        "### Wenn man will, kann man als Gegenwert noch die Accuracy hineinnehmen\n",
        "mse = tf.reduce_mean(tf.square(y - output))\n"
      ],
      "metadata": {
        "id": "Vz6FikS76ZWy"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "with tf.compat.v1.Session() as s:\n",
        "    s.run(tf.compat.v1.global_variables_initializer())\n",
        "    for epoch in range(epochs): \n",
        "      for bb in batch(range(train_inputs.shape[0]), batch_size):\n",
        "        print(type(bb))\n",
        "        s.run(optimizer, feed_dict={ x: train_x[bb], y: train_y[bb] })\n",
        "        train_loss = s.run(cost, feed_dict={\n",
        "                x: train_x[bb],\n",
        "                y: train_y[bb]\n",
        "                })\n",
        "        print('Epoch {:>2}, Batch {} - '\n",
        "        'Training Loss: {:>10.4f}'.format(epoch + 1, bb, train_loss)) \n",
        "        mse_value = s.run(mse, feed_dict={ \n",
        "            x: test_x, y: test_y \n",
        "            }) \n",
        "      print('Mean Squared Error: {}'.format(mse_value)) \n",
        "      print(\"The weights are: {}\".format(s.run(w))) \n",
        "      print(\"and the bias is: {}\".format(s.run(b)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAlZgXmGDUd4",
        "outputId": "ccf1e3c3-3c4c-4531-8db4-dcc6ce8db2c2"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'range'>\n",
            "Epoch  1, Batch range(0, 100) - Training Loss: 32748.9258\n",
            "<class 'range'>\n",
            "Epoch  1, Batch range(100, 200) - Training Loss: 30938.1934\n",
            "<class 'range'>\n",
            "Epoch  1, Batch range(200, 300) - Training Loss: 24891.1914\n",
            "<class 'range'>\n",
            "Epoch  1, Batch range(300, 400) - Training Loss: 26359.3320\n",
            "<class 'range'>\n",
            "Epoch  1, Batch range(400, 500) - Training Loss: 30524.2578\n",
            "<class 'range'>\n",
            "Epoch  1, Batch range(500, 600) - Training Loss: 24169.2578\n",
            "<class 'range'>\n",
            "Epoch  1, Batch range(600, 700) - Training Loss: 24445.6328\n",
            "<class 'range'>\n",
            "Epoch  1, Batch range(700, 750) - Training Loss: 12390.5293\n",
            "Mean Squared Error: 289.0290222167969\n",
            "The weights are: [[ 0.2676849]\n",
            " [-0.8022223]]\n",
            "and the bias is: [1.0819533]\n",
            "<class 'range'>\n",
            "Epoch  2, Batch range(0, 100) - Training Loss: 31854.7051\n",
            "<class 'range'>\n",
            "Epoch  2, Batch range(100, 200) - Training Loss: 30084.0000\n",
            "<class 'range'>\n",
            "Epoch  2, Batch range(200, 300) - Training Loss: 24187.4160\n",
            "<class 'range'>\n",
            "Epoch  2, Batch range(300, 400) - Training Loss: 25633.6562\n",
            "<class 'range'>\n",
            "Epoch  2, Batch range(400, 500) - Training Loss: 29701.3770\n",
            "<class 'range'>\n",
            "Epoch  2, Batch range(500, 600) - Training Loss: 23502.7852\n",
            "<class 'range'>\n",
            "Epoch  2, Batch range(600, 700) - Training Loss: 23758.5742\n",
            "<class 'range'>\n",
            "Epoch  2, Batch range(700, 750) - Training Loss: 12058.4453\n",
            "Mean Squared Error: 281.1730651855469\n",
            "The weights are: [[ 0.40774766]\n",
            " [-0.61151534]]\n",
            "and the bias is: [1.067059]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQWNGqp2VyHM",
        "outputId": "fd821af6-ef89-44ad-8d1e-a2599e9ac014"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable_9:0' shape=(2, 1) dtype=float32>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = numpy.reshape(train_y, (train_y.shape[0], 1))\n",
        "# train_x = numpy.reshape(train_x, (train_x.shape[0], 2))\n",
        "print(train_x.shape, train_y.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9YTVhYXEn_0",
        "outputId": "26414153-b92d-4a97-e2a3-2737fc549f10"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(750, 2) (750, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x.shape, train_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCFydqy1ToQD",
        "outputId": "fc1223d1-f688-4681-96ec-89c65c76bc6a"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(750, 2) (750,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.compat.v1.placeholder(tf.float64, [None, 2])\n",
        "y = tf.compat.v1.placeholder(tf.float64, [None, 1])\n",
        "print(x, y)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1wkoNneMx-1",
        "outputId": "a2bf1bb9-0767-4731-cc15-f32027921463"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"Placeholder_28:0\", shape=(?, 2), dtype=float64) Tensor(\"Placeholder_29:0\", shape=(?, 1), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "### Daten (hier kein Pre-Processing)\n",
        "inp, outp = datasets.make_regression(n_samples=1000, n_features=2, n_informative=2, \n",
        "\tnoise=3, coef=False, random_state=333) \n",
        "\n",
        "#### Default ist eine Auftrennung von 0.75 zu 0.25 Testdatens채tze\n",
        "train_x, test_x, train_y, test_y = train_test_split(inp, outp)\n"
      ],
      "metadata": {
        "id": "6yA9L5OfMlZx"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets\n",
        "### from sklearn.externals import joblib # joblib.dump und joblib.load\n",
        "iris = datasets.load_iris()\n",
        "X= iris.data\n",
        "y= iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "model = DecisionTreeClassifier() # erst Objekt kreieren\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "score = accuracy_score(y_test, predictions)\n"
      ],
      "metadata": {
        "id": "SM925gs2U01H"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_y = numpy.reshape(train_y, (train_y.shape[0], 1))\n",
        "# train_x = numpy.reshape(train_x, (train_x.shape[0], 2))\n",
        "test_y = numpy.reshape(test_y, (test_y.shape[0], 1))\n",
        "print(test_x.shape, test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEusK5LUhWFF",
        "outputId": "8ad32071-8aa8-4f30-8acb-1430bb0b65a5"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250, 2) (250, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "### Daten (hier kein Pre-Processing)\n",
        "inp, outp = datasets.make_regression(n_samples=1000, n_features=2, n_informative=2, \n",
        "\tnoise=3, coef=False, random_state=333) \n",
        "\n",
        "#### Default ist eine Auftrennung von 0.75 zu 0.25 Testdatens채tze\n",
        "train_x, test_x, train_y, test_y = train_test_split(inp, outp)\n"
      ],
      "metadata": {
        "id": "EobWmou2n3FA"
      },
      "execution_count": 191,
      "outputs": []
    }
  ]
}